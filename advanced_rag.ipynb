{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Indexing\n",
    "- Apply sliding window approach to capture local context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "\n",
    "vector_database_path = './data/vector_database.npy'\n",
    "\n",
    "# Load and process the PDF document\n",
    "def load_document(file_path):\n",
    "    text = textract.process(file_path, method='pdfminer').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "# Segment text using a sliding window approach\n",
    "def segment_text(text, window_size=500, step=250):\n",
    "    return [text[i:i+window_size] for i in range(0, len(text)-window_size+1, step)]\n",
    "\n",
    "# Create TF-IDF based index\n",
    "def create_and_save_index(segments, filename=vector_database_path):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(segments)\n",
    "\n",
    "    # Save the matrix and vectorizer\n",
    "    np.save(filename, tfidf_matrix.toarray())\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "# Load index from file\n",
    "def load_index(filename=vector_database_path):\n",
    "    matrix = np.load(filename)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Enhancing the query by re-writing or through expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance query, expand or re-write\n",
    "def enhance_query (query):\n",
    "    # May involve adding keywords, synonyms, domain specific terms or something similar\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Retrieval\n",
    "- Using the enhanced query, retrieve relevant document segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, vectorizer, tfidf_matrix, top_k=5):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    cosine_similarities = linear_kernel(query_vec, tfidf_matrix).flatten()\n",
    "    top_indices = cosine_similarities.argsort()[-top_k:][::-1]\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Post retrieval\n",
    "- Process the retrieved content to highlight the most relevant information\n",
    "- Prepare the content for the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_retrieved_data(indices, segments):\n",
    "    # This could involve summarizing or selecting key segments\n",
    "    # For simplicity, I concatenate the top segments\n",
    "    return \" \".join([segments[i] for i in indices]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generation\n",
    "- Put the prompt together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def generate_response(prompt, api_key):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(messages=[{ \"role\":\"user\", \"content\": prompt }], model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    generated_text = response.choices[0].message.content\n",
    "\n",
    "    # Saving the response\n",
    "    file_path = './data/advanced-rag-chat-gpt-response.txt'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(generated_text)\n",
    "    \n",
    "    print('Response saved in: ')\n",
    "    print(file_path)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running actions now\n",
    "- Get OpenAI key\n",
    "- Prepare document\n",
    "- And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response saved in: \n",
      "./data/advanced-rag-chat-gpt-response.txt\n",
      "Back propagation in neural networks refers to the method used to compute the gradient of the loss function with respect to the weights of the network. It is a key component of training a neural network through techniques like stochastic gradient descent. \n",
      "\n",
      "The process involves passing the input data forward through the network to make a prediction, calculating the error between the predicted output and the true output (loss function), and then propagating this error backward through the network to update the weights in order to minimize the error.\n",
      "\n",
      "During back propagation, the gradient of the loss function is computed with respect to each weight in the network using the chain rule of calculus. This gradient is then used to update the weights in the direction that minimizes the error, effectively optimizing the network for better performance.\n",
      "\n",
      "Overall, back propagation is a fundamental concept in training neural networks and plays a crucial role in enabling deep learning models to learn from data and make accurate predictions.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = './data/deep-learning.pdf'\n",
    "api_key = os.getenv('OPEN_AI_KEY')\n",
    "query = \"Explain the concept of back propagation in neural networks\"\n",
    "\n",
    "text = load_document(file_path=file_path)\n",
    "segments = segment_text(text=text)\n",
    "vectorizer , tfidf_matrix = create_and_save_index(segments=segments)\n",
    "tfidf = load_index() # Replaced with communication with vector database\n",
    "\n",
    "\n",
    "enhanced_query = enhance_query(query=query)\n",
    "indices = retrieve_documents(enhanced_query, vectorizer=vectorizer, tfidf_matrix=tfidf_matrix)\n",
    "processed_text = process_retrieved_data(indices=indices, segments=segments)\n",
    "prompt = f\"Question: {query}\\nContext: {processed_text}\"\n",
    "response = generate_response(prompt=prompt, api_key=api_key)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
