{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "- Clean and extract text\n",
    "- Segment text into chunks\n",
    "- Encode these chunks into vectors\n",
    "- Store vectors in databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import textract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model for encoding text into vectors\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk:\n",
      "atrices\n",
      "2.3\n",
      ". . . . . . . . . . . . . . . . . . . .\n",
      "2.4\n",
      "Linear Dependence and Span . . . . . . . . . . . . . . . . . . . .\n",
      "Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "2.5\n",
      "Special Kinds of Matrices and Vectors\n",
      "2.6\n",
      ". . . . . . . . . . . . . . .\n",
      "2.7\n",
      "Eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Singular Value Decomposition . . . . . . . . . . . . . . . . . . . .\n",
      "2.8\n",
      "The Moore-Penrose Pseudoinverse . . . . . . . . . . . . . . . . . .\n",
      "2.9\n",
      "2.10 The \n"
     ]
    }
   ],
   "source": [
    "# Extract text from a PDF (Sample source)\n",
    "text = textract.process(\"./data/deep-learning.pdf\", method=\"pdfminer\").decode()\n",
    "\n",
    "# Segment the text into chunks\n",
    "chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "print('Sample chunk:')\n",
    "print(chunks[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "# Encoding and indexing\n",
    "dim = model.config.hidden_size\n",
    "index = faiss.IndexFlatL2(dim) # Using L2 distance for simplicity\n",
    "\n",
    "for chunk in chunks:\n",
    "    vec = encode_text(chunk)\n",
    "    index.add(vec)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"./data/store.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling retrieval\n",
    "- Encode the user query\n",
    "- Compute similarity scores between the query vector and document vectors\n",
    "- Retrieve the top K similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved chunks:\n",
      "['orld. For example, Cyc failed to understand a story\\nabout a person named Fred shaving in the morning (\\n). Its inference\\nengine detected an inconsistency in the story: it knew that people do not have\\nelectrical parts, but because Fred was holding an electric razor, it believed the\\nentity “FredWhileShaving” contained electrical parts. It therefore asked whether\\nFred was still a person while he was shaving.\\n\\nLinde 1992\\n\\n,\\n\\nThe diﬃculties faced by systems relying on hard-coded knowledge suggest\\nthat', 'ge and acquiring knowledge can be done via learning,\\nwhich has motivated the development of large-scale deep architectures. However,\\nthere are diﬀerent kinds of knowledge. Some knowledge can be implicit, sub-\\nconscious, and diﬃcult to verbalize—such as how to walk, or how a dog looks\\ndiﬀerent from a cat. Other knowledge can be explicit, declarative, and relatively\\nstraightforward to put into words—every day commonsense knowledge, like “a cat\\nis a kind of animal,” or very speciﬁc facts that you n', 'ves\\nmultiple quadratic ﬁlters for each neuron (\\n). Indeed our\\ncartoon picture of “simple cells” and “complex cells” might create a non-\\nexistent distinction; simple cells and complex cells might both be the same\\nkind of cell but with their “parameters” enabling a continuum of behaviors\\nranging from what we call “simple” to what we call “complex.”\\n\\nRust et al. 2005\\n\\n,\\n\\nIt is also worth mentioning that neuroscience has told us relatively little\\nabout how to train convolutional networks. Model stru', 'e not yet learned to\\nﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\\nincluding the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\\ncommunicate, and after all of this eﬀort is still very brittle and prone to failure.\\n\\nWhile it should be clear that we need a means of representing and reasoning\\nabout uncertainty, it is not immediately obvious that probability theory can provide\\nall of the tools we want for artiﬁcial intelligence applic', ' as legendary inventors, and\\nGalatea, Talos, and Pandora may all be regarded as artiﬁcial life (\\nOvid and Martin\\n,\\n2004 Sparkes 1996 Tandy 1997\\n\\n).\\n\\n,\\n\\n;\\n\\n,\\n\\n;\\n\\nWhen programmable computers were ﬁrst conceived, people wondered whether\\nsuch machines might become intelligent, over a hundred years before one was\\n). Today, artiﬁcial intelligence (AI) is a thriving ﬁeld with\\nbuilt (Lovelace 1842\\nmany practical applications and active research topics. We look to intelligent\\nsoftware to automate routine'] [30.075592 30.096832 30.153702 30.631277 30.65487 ]\n"
     ]
    }
   ],
   "source": [
    "# Handling retrieval\n",
    "def retrieve(query, k=5):\n",
    "    query_vec = encode_text(query)\n",
    "    D, I = index.search(query_vec, k)\n",
    "    return [chunks[i] for i in I[0]], D[0]\n",
    "\n",
    "# Example\n",
    "query = \"What is RAG in AI?\"\n",
    "retrieved_chunks, distances = retrieve(query=query)\n",
    "print(\"Retrieved chunks:\")\n",
    "print(retrieved_chunks, distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n",
    "- Combine the query and retrieved texts into a coherent prompt\n",
    "- Generate a response using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model facebook/blenderbot-400M-distill with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 737, in _error_catcher\n    yield\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 883, in _raw_read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\nurllib3.exceptions.IncompleteRead: IncompleteRead(273881815 bytes read, 455874168 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/requests/models.py\", line 816, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 1043, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 963, in read\n    data = self._raw_read(amt)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 861, in _raw_read\n    with self._error_catcher():\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 761, in _error_catcher\n    raise ProtocolError(arg, e) from e\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(273881815 bytes read, 455874168 more expected)', IncompleteRead(273881815 bytes read, 455874168 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3335, in from_pretrained\n    resolved_archive_file = cached_file(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 539, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/requests/models.py\", line 818, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(273881815 bytes read, 455874168 more expected)', IncompleteRead(273881815 bytes read, 455874168 more expected))\n\nwhile loading with BlenderbotForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 737, in _error_catcher\n    yield\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 883, in _raw_read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\nurllib3.exceptions.IncompleteRead: IncompleteRead(371444828 bytes read, 85681395 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/requests/models.py\", line 816, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 1043, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 963, in read\n    data = self._raw_read(amt)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 861, in _raw_read\n    with self._error_catcher():\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 761, in _error_catcher\n    raise ProtocolError(arg, e) from e\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(371444828 bytes read, 85681395 more expected)', IncompleteRead(371444828 bytes read, 85681395 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_blenderbot.py\", line 1210, in from_pretrained\n    return super(BlenderbotForConditionalGeneration, cls).from_pretrained(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3335, in from_pretrained\n    resolved_archive_file = cached_file(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 539, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/requests/models.py\", line 818, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(371444828 bytes read, 85681395 more expected)', IncompleteRead(371444828 bytes read, 85681395 more expected))\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacebook/blenderbot-400M-distill\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(query, retrieved_chunks):\n\u001b[1;32m      7\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(retrieved_chunks)\n",
      "File \u001b[0;32m~/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/pipelines/base.py:296\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    295\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model facebook/blenderbot-400M-distill with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 737, in _error_catcher\n    yield\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 883, in _raw_read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\nurllib3.exceptions.IncompleteRead: IncompleteRead(273881815 bytes read, 455874168 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/requests/models.py\", line 816, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 1043, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 963, in read\n    data = self._raw_read(amt)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 861, in _raw_read\n    with self._error_catcher():\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 761, in _error_catcher\n    raise ProtocolError(arg, e) from e\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(273881815 bytes read, 455874168 more expected)', IncompleteRead(273881815 bytes read, 455874168 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3335, in from_pretrained\n    resolved_archive_file = cached_file(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 539, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/requests/models.py\", line 818, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(273881815 bytes read, 455874168 more expected)', IncompleteRead(273881815 bytes read, 455874168 more expected))\n\nwhile loading with BlenderbotForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 737, in _error_catcher\n    yield\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 883, in _raw_read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\nurllib3.exceptions.IncompleteRead: IncompleteRead(371444828 bytes read, 85681395 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/requests/models.py\", line 816, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 1043, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 963, in read\n    data = self._raw_read(amt)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 861, in _raw_read\n    with self._error_catcher():\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/urllib3/response.py\", line 761, in _error_catcher\n    raise ProtocolError(arg, e) from e\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(371444828 bytes read, 85681395 more expected)', IncompleteRead(371444828 bytes read, 85681395 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_blenderbot.py\", line 1210, in from_pretrained\n    return super(BlenderbotForConditionalGeneration, cls).from_pretrained(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3335, in from_pretrained\n    resolved_archive_file = cached_file(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 539, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/requests/models.py\", line 818, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(371444828 bytes read, 85681395 more expected)', IncompleteRead(371444828 bytes read, 85681395 more expected))\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "def generate_response(query, retrieved_chunks):\n",
    "    prompt = f\"Question: {query}\\n\\nContext: \" + \" \".join(retrieved_chunks)\n",
    "    total_length = len(tokenizer.encode(prompt)) + 250\n",
    "    response = generator(prompt, truncation=True, max_length=total_length, num_return_sequences=1)\n",
    "    generated_response = response[0]['generated_text']\n",
    "\n",
    "    # Saving the response\n",
    "    file_path = './data/response.txt'\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(generated_response)\n",
    "    \n",
    "    return generated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is RAG in AI?\n",
      "\n",
      "Context: orld. For example, Cyc failed to understand a story\n",
      "about a person named Fred shaving in the morning (\n",
      "). Its inference\n",
      "engine detected an inconsistency in the story: it knew that people do not have\n",
      "electrical parts, but because Fred was holding an electric razor, it believed the\n",
      "entity “FredWhileShaving” contained electrical parts. It therefore asked whether\n",
      "Fred was still a person while he was shaving.\n",
      "\n",
      "Linde 1992\n",
      "\n",
      ",\n",
      "\n",
      "The diﬃculties faced by systems relying on hard-coded knowledge suggest\n",
      "that ge and acquiring knowledge can be done via learning,\n",
      "which has motivated the development of large-scale deep architectures. However,\n",
      "there are diﬀerent kinds of knowledge. Some knowledge can be implicit, sub-\n",
      "conscious, and diﬃcult to verbalize—such as how to walk, or how a dog looks\n",
      "diﬀerent from a cat. Other knowledge can be explicit, declarative, and relatively\n",
      "straightforward to put into words—every day commonsense knowledge, like “a cat\n",
      "is a kind of animal,” or very speciﬁc facts that you n ves\n",
      "multiple quadratic ﬁlters for each neuron (\n",
      "). Indeed our\n",
      "cartoon picture of “simple cells” and “complex cells” might create a non-\n",
      "existent distinction; simple cells and complex cells might both be the same\n",
      "kind of cell but with their “parameters” enabling a continuum of behaviors\n",
      "ranging from what we call “simple” to what we call “complex.”\n",
      "\n",
      "Rust et al. 2005\n",
      "\n",
      ",\n",
      "\n",
      "It is also worth mentioning that neuroscience has told us relatively little\n",
      "about how to train convolutional networks. Model stru e not yet learned to\n",
      "ﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\n",
      "including the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\n",
      "communicate, and after all of this eﬀort is still very brittle and prone to failure.\n",
      "\n",
      "While it should be clear that we need a means of representing and reasoning\n",
      "about uncertainty, it is not immediately obvious that probability theory can provide\n",
      "all of the tools we want for artiﬁcial intelligence applic  as legendary inventors, and\n",
      "Galatea, Talos, and Pandora may all be regarded as artiﬁcial life (\n",
      "Ovid and Martin\n",
      ",\n",
      "2004 Sparkes 1996 Tandy 1997\n",
      "\n",
      ").\n",
      "\n",
      ",\n",
      "\n",
      ";\n",
      "\n",
      ",\n",
      "\n",
      ";\n",
      "\n",
      "When programmable computers were ﬁrst conceived, people wondered whether\n",
      "such machines might become intelligent, over a hundred years before one was\n",
      "). Today, artiﬁcial intelligence (AI) is a thriving ﬁeld with\n",
      "built (Lovelace 1842\n",
      "many practical applications and active research topics. We look to intelligent\n",
      "software to automate routine\n",
      "\n",
      "programs.\n",
      "\n",
      "ﬀng a new generation of people\n",
      "\n",
      "has made their way into the field (Tandy 1997). These\n",
      "\n",
      "people can take it to the next level\n",
      "\n",
      ";\n",
      "\n",
      ";\n",
      "\n",
      ";\n",
      "\n",
      "the possibilities are deep and open ( Lovelace 1842\n",
      "\n",
      "the world´s foremost scientific publication about\n",
      "\n",
      "science \"\n",
      "\n",
      "); as well as there exists a\n",
      "\n",
      "new genre of the artist; it has\n",
      "\n",
      "also revolutionized other fields such as\n",
      "\n",
      "fiction, politics, art, science,\n",
      "\n",
      "and art history.\n",
      "\n",
      "ﬀng an international movement\n",
      "\n",
      "of artists using\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running actions\n",
    "response = generate_response(query, retrieved_chunks=retrieved_chunks)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
