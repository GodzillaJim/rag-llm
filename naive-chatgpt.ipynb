{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "- Clean and extract text\n",
    "- Segment text into chunks\n",
    "- Encode these chunks into vectors\n",
    "- Store vectors in databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model for encoding text into vectors\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from a PDF (Sample source)\n",
    "text = textract.process(\"./data/deep-learning.pdf\", method=\"pdfminer\").decode()\n",
    "\n",
    "# Segment the text into chunks\n",
    "chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "print('Sample chunk:')\n",
    "print(chunks[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and indexing\n",
    "dim = model.config.hidden_size\n",
    "index = faiss.IndexFlatL2(dim) # Using L2 distance for simplicity\n",
    "\n",
    "for chunk in chunks:\n",
    "    vec = encode_text(chunk)\n",
    "    index.add(vec)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"./data/store-chatgpt.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling retrieval\n",
    "- Encode the user query\n",
    "- Compute similarity scores between the query vector and document vectors\n",
    "- Retrieve the top K similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling retrieval\n",
    "def retrieve(query, k=5):\n",
    "    # Load index from file, this is replaced with a vector database\n",
    "    index = faiss.read_index('./data/store-chatgpt.faiss')\n",
    "    query_vec = encode_text(query)\n",
    "    D, I = index.search(query_vec, k)\n",
    "    return [chunks[i] for i in I[0]], D[0]\n",
    "\n",
    "# Example\n",
    "query = \"What is RAG in AI?\"\n",
    "retrieved_chunks, distances = retrieve(query=query)\n",
    "print(\"Retrieved chunks:\")\n",
    "print(retrieved_chunks, distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n",
    "- Combine the query and retrieved texts into a coherent prompt\n",
    "- Generate a response using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def generate_response_with_chatgpt(query, retrieved_chunks, api_key):\n",
    "    prompt = f\"Question: {query}\\nContext: \" + \" \".join(retrieved_chunks)\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(messages=[{ \"role\":\"user\", \"content\": prompt }], model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    generated_text = response.choices[0].message.content\n",
    "\n",
    "    # Saving the response\n",
    "    file_path = './data/chat-gpt-response.txt'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(generated_text)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Calling actions\n",
    "api_key = os.getenv('OPEN_AI_KEY')\n",
    "query = \"Explain the concept of back propagation in neural networks\"\n",
    "retrieved_chunks, _ = retrieve(query)\n",
    "response = generate_response_with_chatgpt(query, retrieved_chunks=retrieved_chunks, api_key=api_key)\n",
    "\n",
    "print('Saved response in text file: ./data/chat-gpt-response.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
