{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "- Clean and extract text\n",
    "- Segment text into chunks\n",
    "- Encode these chunks into vectors\n",
    "- Store vectors in databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model for encoding text into vectors\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk:\n",
      "atrices\n",
      "2.3\n",
      ". . . . . . . . . . . . . . . . . . . .\n",
      "2.4\n",
      "Linear Dependence and Span . . . . . . . . . . . . . . . . . . . .\n",
      "Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "2.5\n",
      "Special Kinds of Matrices and Vectors\n",
      "2.6\n",
      ". . . . . . . . . . . . . . .\n",
      "2.7\n",
      "Eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Singular Value Decomposition . . . . . . . . . . . . . . . . . . . .\n",
      "2.8\n",
      "The Moore-Penrose Pseudoinverse . . . . . . . . . . . . . . . . . .\n",
      "2.9\n",
      "2.10 The \n"
     ]
    }
   ],
   "source": [
    "# Extract text from a PDF (Sample source)\n",
    "text = textract.process(\"./data/deep-learning.pdf\", method=\"pdfminer\").decode()\n",
    "\n",
    "# Segment the text into chunks\n",
    "chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "print('Sample chunk:')\n",
    "print(chunks[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/jimna/Desktop/projects/rag/ragenv/lib/python3.10/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "# Encoding and indexing\n",
    "dim = model.config.hidden_size\n",
    "index = faiss.IndexFlatL2(dim) # Using L2 distance for simplicity\n",
    "\n",
    "for chunk in chunks:\n",
    "    vec = encode_text(chunk)\n",
    "    index.add(vec)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"./data/store-chatgpt.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling retrieval\n",
    "- Encode the user query\n",
    "- Compute similarity scores between the query vector and document vectors\n",
    "- Retrieve the top K similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved chunks:\n",
      "['orld. For example, Cyc failed to understand a story\\nabout a person named Fred shaving in the morning (\\n). Its inference\\nengine detected an inconsistency in the story: it knew that people do not have\\nelectrical parts, but because Fred was holding an electric razor, it believed the\\nentity “FredWhileShaving” contained electrical parts. It therefore asked whether\\nFred was still a person while he was shaving.\\n\\nLinde 1992\\n\\n,\\n\\nThe diﬃculties faced by systems relying on hard-coded knowledge suggest\\nthat', 'ge and acquiring knowledge can be done via learning,\\nwhich has motivated the development of large-scale deep architectures. However,\\nthere are diﬀerent kinds of knowledge. Some knowledge can be implicit, sub-\\nconscious, and diﬃcult to verbalize—such as how to walk, or how a dog looks\\ndiﬀerent from a cat. Other knowledge can be explicit, declarative, and relatively\\nstraightforward to put into words—every day commonsense knowledge, like “a cat\\nis a kind of animal,” or very speciﬁc facts that you n', 'ves\\nmultiple quadratic ﬁlters for each neuron (\\n). Indeed our\\ncartoon picture of “simple cells” and “complex cells” might create a non-\\nexistent distinction; simple cells and complex cells might both be the same\\nkind of cell but with their “parameters” enabling a continuum of behaviors\\nranging from what we call “simple” to what we call “complex.”\\n\\nRust et al. 2005\\n\\n,\\n\\nIt is also worth mentioning that neuroscience has told us relatively little\\nabout how to train convolutional networks. Model stru', 'e not yet learned to\\nﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\\nincluding the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\\ncommunicate, and after all of this eﬀort is still very brittle and prone to failure.\\n\\nWhile it should be clear that we need a means of representing and reasoning\\nabout uncertainty, it is not immediately obvious that probability theory can provide\\nall of the tools we want for artiﬁcial intelligence applic', ' as legendary inventors, and\\nGalatea, Talos, and Pandora may all be regarded as artiﬁcial life (\\nOvid and Martin\\n,\\n2004 Sparkes 1996 Tandy 1997\\n\\n).\\n\\n,\\n\\n;\\n\\n,\\n\\n;\\n\\nWhen programmable computers were ﬁrst conceived, people wondered whether\\nsuch machines might become intelligent, over a hundred years before one was\\n). Today, artiﬁcial intelligence (AI) is a thriving ﬁeld with\\nbuilt (Lovelace 1842\\nmany practical applications and active research topics. We look to intelligent\\nsoftware to automate routine'] [30.075592 30.096832 30.153702 30.631277 30.65487 ]\n"
     ]
    }
   ],
   "source": [
    "# Handling retrieval\n",
    "def retrieve(query, k=5):\n",
    "    # Load index from file, this is replaced with a vector database\n",
    "    index = faiss.read_index('./data/store-chatgpt.faiss')\n",
    "    query_vec = encode_text(query)\n",
    "    D, I = index.search(query_vec, k)\n",
    "    return [chunks[i] for i in I[0]], D[0]\n",
    "\n",
    "# Example\n",
    "query = \"What is RAG in AI?\"\n",
    "retrieved_chunks, distances = retrieve(query=query)\n",
    "print(\"Retrieved chunks:\")\n",
    "print(retrieved_chunks, distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n",
    "- Combine the query and retrieved texts into a coherent prompt\n",
    "- Generate a response using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAIKey: \n",
      "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\n",
      "Saved response in text file: ./data/chat-gpt-response.txt\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def generate_response_with_chatgpt(query, retrieved_chunks, api_key):\n",
    "    prompt = f\"Question: {query}\\nContext: \" + \" \".join(retrieved_chunks)\n",
    "\n",
    "    print('OpenAIKey: ')\n",
    "    print(api_key)\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(messages=[{ \"role\":\"user\", \"content\": prompt }], model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    generated_text = response.choices[0].message.content\n",
    "\n",
    "    # Saving the response\n",
    "    file_path = './data/chat-gpt-response.txt'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(generated_text)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Calling actions\n",
    "api_key = os.getenv('OPEN_AI_KEY')\n",
    "query = \"Explain the concept of back propagation in neural networks\"\n",
    "retrieved_chunks, _ = retrieve(query)\n",
    "response = generate_response_with_chatgpt(query, retrieved_chunks=retrieved_chunks, api_key=api_key)\n",
    "\n",
    "print('Saved response in text file: ./data/chat-gpt-response.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
