{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Document processing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vector_database_path = './data/modular_rag_vector_database.npy'\n",
    "raw_file_path = './data/deep-learning.pdf'\n",
    "\n",
    "# Segment text using a sliding window approach\n",
    "def segment_text(text, window_size=500, step=250):\n",
    "    return [text[i:i+window_size] for i in range(0, len(text)-window_size+1, step)]\n",
    "\n",
    "# Load data, mock storage\n",
    "def load_and_process_document(file_path=raw_file_path):\n",
    "    text = textract.process(file_path, method=\"pdfminer\").decode('utf-8')\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "    segments = segment_text(text)\n",
    "    tfidf_matrix = vectorizer.fit_transform(segments)\n",
    "    np.save(vector_database_path, tfidf_matrix.toarray())\n",
    "    return vectorizer, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement routing to optimize retrieval\n",
    "- FIrst infer intent of query. Working with these classes : 'history', 'forecast' & 'reasoning'\n",
    "- This involves training a model to infer the classes\n",
    "- Another approach would be to use an LLM for the classification\n",
    "- Once we can classify the query, we determine actions to take for each class. In this case, I am appending \n",
    "additional keywords to the query for better targeting in retrieval. Another option may include switching databases or using fewer resources.\n",
    "- Other ways for more complex use cases may involve semantic routes instead of if statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing an automated routing system\n",
    "# First infer intent of query \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "model_path = './model/query_classifier_model.pkl'\n",
    "\n",
    "data = [\n",
    "    {\"query\": \"What led to the adoption of CNNs?\", \"label\": \"history\"},\n",
    "    {\"query\": \"Will GANs improve image synthesis in the future?\", \"label\": \"forecast\"},\n",
    "    {\"query\": \"Why do dropout layers help reduce overfitting?\", \"label\": \"reasoning\"}\n",
    "    # Add more data for better accuracy\n",
    "]\n",
    "\n",
    "# Prepare dataset\n",
    "texts = [item['query'] for item in data]\n",
    "labels = [item['label'] for item in data]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create ML pipeline\n",
    "pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('classifier', LogisticRegression())])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Other training stages such as testing, feedback and improvement apply\n",
    "# Save the model\n",
    "joblib.dump(pipeline, model_path)\n",
    "\n",
    "# Function to classify the query\n",
    "# Return 0: history, 1:forecast, 2:reasoning\n",
    "def classify_query(query, model):\n",
    "    prediction = model.predict([query])[0]\n",
    "    return {'history': 0, 'forecast': 1, 'reasoning': 2}[prediction]\n",
    "\n",
    "def route_query(query, model):\n",
    "    intent = classify_query(query=query, model=model)\n",
    "    \n",
    "    if intent == 0:\n",
    "        # The actions here are open.\n",
    "        return f\"Historical data for: {query}\"\n",
    "    elif intent == 1:\n",
    "        return f\"Future events {query}\"\n",
    "    else:\n",
    "        return f\"Formulae, infer, calculate ${query}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Search vectors\n",
    "- We search the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search_vectors(query, vectorizer, filename=vector_database_path):\n",
    "    vectors = np.load(filename)\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vec, vectors).flatten()\n",
    "    top_k = scores.argsort()[-5:][::-1]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def generate_response(prompt, api_key):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(messages=[{ \"role\":\"user\", \"content\": prompt }], model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    generated_text = response.choices[0].message.content\n",
    "\n",
    "    # Saving the response\n",
    "    file_path = './data/modular-rag-chat-gpt-response.txt'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(generated_text)\n",
    "    \n",
    "    print('Response saved in: ')\n",
    "    print(file_path)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = './data/deep-learning.pdf'\n",
    "api_key = os.getenv('OPEN_AI_KEY')\n",
    "query = \"Explain the concept of back propagation in neural networks\"\n",
    "\n",
    "# Process queries\n",
    "model = joblib.load(model_path)\n",
    "processed_query = route_query(query=query, model=model)\n",
    "print('ProcessedQuery: ')\n",
    "print(processed_query)\n",
    "\n",
    "# Search vectors\n",
    "vectorizer, _ = load_and_process_document()\n",
    "context = search_vectors(processed_query, vectorizer=vectorizer)\n",
    "\n",
    "# Generate response\n",
    "prompt = f\"Question: {processed_query}\\nContext: {context}\"\n",
    "response = generate_response(prompt=prompt, api_key=api_key)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
